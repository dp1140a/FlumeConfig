{
    "name": "HDFSSink",
    "id": "",
    "printName": "HDFS",
    "description": "This sink writes events into the Hadoop Distributed File System (HDFS). It currently supports creating text and sequence files. It supports compression in both file types. The files can be rolled (close current file and create a new one) periodically based on the elapsed time or size of data or number of events. It also buckets/partitions data by attributes like timestamp or machine where the event originated. The HDFS directory path may contain formatting escape sequences that will replaced by the HDFS sink to generate a directory/file name to store the events. Using this sink requires hadoop to be installed so that Flume can use the Hadoop jars to communicate with the HDFS cluster. Note that a version of Hadoop that supports the sync() call is required.",
    "type": "hdfs",
    "category": "sinks",
    "configProperties": {
        "hdfsPath": {
            "name": "hdfs.path",
            "printName": "HDFS Path",
            "value": "",
            "valueType": "String",
            "required": true,
            "description": "HDFS directory path (eg hdfs://namenode/flume/webdata/)"
        },
        "hdfsFilePrefix": {
            "name": "hdfs.filePrefix",
            "printName": "HDFS File Prefix",
            "value": "FlumeData",
            "valueType": "String",
            "required": false,
            "description": "Name prefixed to files created by Flume in hdfs directory"
        },
        "hdfsFileSuffix": {
            "name": "hdfs.fileSuffix",
            "printName": "HDFS File Suffix",
            "value": "",
            "valueType": "String",
            "required": false,
            "description": "Suffix to append to file (eg .avro - NOTE: period is not automatically added)"
        },
        "hdfsInUsePrefix": {
            "name": "hdfs.inUsePrefix",
            "printName": "HDFS In Use Prefix",
            "value": "",
            "valueType": "String",
            "required": false,
            "description": "Prefix that is used for temporal files that flume actively writes into"
        },
        "hdfsInUsesuffix": {
            "name": "hdfs.inUsePrefix",
            "printName": "HDFS In Use Suffix",
            "value": ".tmp",
            "valueType": "String",
            "required": false,
            "description": "Suffix that is used for temporal files that flume actively writes into"
        },
        "hdfsRollInterval": {
            "name": "hdfs.rollInterval",
            "printName": "HDFS Roll Interval",
            "value": 0,
            "valueType": "Number",
            "required": false,
            "description": "Number of seconds to wait before rolling current file (0 = never roll based on time interval)"
        },
        "hdfsRollSize": {
            "name": "hdfs.rollSize",
            "printName": "HDFS Roll Size",
            "value": 1024,
            "valueType": "Number",
            "required": false,
            "description": "File size to trigger roll, in bytes (0: never roll based on file size)"
        },
        "hdfsRollCount": {
            "name": "hdfs.rollCount",
            "printName": "HDFS Roll Count",
            "value": 10,
            "valueType": "Number",
            "required": false,
            "description": "Number of events written to file before it rolled (0 = never roll based on number of events)"
        },
        "hdfsIdleTimeout": {
            "name": "hdfs.idleTimeout",
            "printName": "HDFS Idle Timeout",
            "value": 0,
            "valueType": "Number",
            "required": false,
            "description": "Timeout after which inactive files get closed (0 = disable automatic closing of idle files)"
        },
        "hdfsBatchSize": {
            "name": "hdfs.batchSize",
            "printName": "HDFS Batch Size",
            "value": 100,
            "valueType": "Number",
            "required": false,
            "description": "number of events written to file before it is flushed to HDFS"
        },
        "hdfsCodec": {
            "name": "hdfs.codeC",
            "printName": "HDFS Codec",
            "value": "",
            "valueType": "String",
            "required": false,
            "description": "Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy"
        },
        "hdfsFileType": {
            "name": "hdfs.fileType",
            "printName": "HDFS File Type",
            "value": "SequenceFile",
            "valueType": "String",
            "required": false,
            "description": "File format: currently SequenceFile, DataStream or CompressedStream (1)DataStream will not compress output file and please donâ€™t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC"
        },
        "hdfsMaxOpenFiles": {
            "name": "hdfs.maxOpenFiles",
            "printName": "HDFS Max Open Files",
            "value": 5000,
            "valueType": "Number",
            "required": false,
            "description": "Allow only this number of open files. If this number is exceeded, the oldest file is closed."
        },
        "hdfsMinBlockReplica": {
            "name": "hdfs.minBlockReplica",
            "printName": "HDFS Min Block Replica",
            "value": null,
            "valueType": "Number",
            "required": false,
            "description": "Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath."
        }
    }
}